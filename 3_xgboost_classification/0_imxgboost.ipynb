{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced XGBoost\n",
    "--------------\n",
    "변경사항\n",
    "- 1) current_date\n",
    "- 2) outcome_name ; 변경하면서 다시 재실행 \n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./../{}'.format(\"config.json\")) as file:\n",
    "    cfg = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = cfg[\"working_date\"]\n",
    "# outcome_list=['meloxicam', 'celecoxib', 'valproic_acid, lamotrigine']\n",
    "outcome_name = 'meloxicam' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "current_dir = pathlib.Path.cwd()\n",
    "parent_dir = current_dir.parent\n",
    "data_dir = pathlib.Path('{}/data/{}/preprocess/{}/'.format(parent_dir, current_date, outcome_name))\n",
    "output_result_dir = pathlib.Path('{}/result/{}/imxgboost/{}/'.format(parent_dir, current_date, outcome_name))\n",
    "pathlib.Path.mkdir(output_result_dir, mode=0o777, parents=True, exist_ok=True)\n",
    "\n",
    "file_list = os.listdir(data_dir)\n",
    "file_list = [pathlib.Path(filename).with_suffix('').name for filename in file_list]\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, cross_validate, GridSearchCV\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from model_estimation import *\n",
    "import functools\n",
    "\n",
    "concat_df = pd.read_csv('{}/{}.txt'.format(data_dir, outcome_name), index_col=False)\n",
    "\n",
    "concat_df['cohort_start_date'] = pd.to_datetime(concat_df['cohort_start_date'], format='%Y-%m-%d %H:%M:%S', errors='raise')\n",
    "concat_df['first_abnormal_date'] = pd.to_datetime(concat_df['first_abnormal_date'], format='%Y-%m-%d %H:%M:%S', errors='raise')\n",
    "concat_df['concept_date'] = pd.to_datetime(concat_df['concept_date'], format='%Y-%m-%d %H:%M:%S', errors='raise')\n",
    "    \n",
    "# concat_df['duration'] = (concat_df['concept_date']-concat_df['cohort_start_date']).dt.days+1\n",
    "concat_df = concat_df.drop(['person_id', 'cohort_start_date', 'concept_date', 'first_abnormal_date'], axis=1)\n",
    "\n",
    "### @change column name ; column에 json파일 구분자가 들어가면 plot을 그리지 못함. \n",
    "import re\n",
    "concat_df.columns = concat_df.columns.str.translate(\"\".maketrans({\"[\":\"(\", \"]\":\")\"}))\n",
    "concat_df = concat_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_ /()]+', '', x))\n",
    "concat_df.columns\n",
    "\n",
    "### @환자수 확인\n",
    "print(\"label_1 : \",len(concat_df[concat_df[\"label\"] == 1]))\n",
    "print(\"label_0 : \",len(concat_df[concat_df[\"label\"] == 0]))\n",
    "\n",
    "### @x, y데이터 분할 \n",
    "def split_x_y_data(df) :\n",
    "    y_data = df['label'].T.reset_index(drop=True) \n",
    "    x_data = df.drop('label', axis=1)\n",
    "    new_col = x_data.columns\n",
    "    return x_data, y_data, new_col\n",
    "\n",
    "x_data, y_data, new_col = split_x_y_data(concat_df)\n",
    "\n",
    "### @train/test dataset 구분 \n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=1, stratify=y_data)\n",
    "\n",
    "print(\"data  : \", x_data.shape, y_data.shape)\n",
    "print(\"train : \", x_train.shape, y_train.shape)\n",
    "print(\"test  : \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### @ imblanced xgboost\n",
    "\n",
    "xgboster_focal = imb_xgb(special_objective='focal', num_round=200, max_depth=5, eta=0.1)\n",
    "xgboster_weight = imb_xgb(special_objective='weighted')\n",
    "CV_focal_booster = GridSearchCV(xgboster_focal, {\"focal_gamma\":[1.0,1.5,2.0,2.5,3.0]})\n",
    "CV_weight_booster = GridSearchCV(xgboster_weight, {\"imbalance_alpha\":[1.5,2.0,2.5,3.0,4.0]})\n",
    "\n",
    "CV_focal_booster.fit(x_train.to_numpy(), y_train.to_numpy())\n",
    "CV_weight_booster.fit(x_train.to_numpy(), y_train.to_numpy())\n",
    "\n",
    "opt_focal_booster = CV_focal_booster.best_estimator_\n",
    "opt_weight_booster = CV_weight_booster.best_estimator_\n",
    "\n",
    "raw_output = opt_focal_booster.predict(x_test.to_numpy(), y=None)\n",
    "rmse = np.sqrt(mean_squared_error(y_test.to_numpy(), raw_output))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "sigmoid_output = opt_focal_booster.predict_sigmoid(x_test.to_numpy(), y=None) \n",
    "rmse = np.sqrt(mean_squared_error(y_test.to_numpy(), sigmoid_output))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "class_output = opt_focal_booster.predict_determine(x_test.to_numpy(), y=None) \n",
    "rmse = np.sqrt(mean_squared_error(y_test.to_numpy(), class_output))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "prob_output = opt_focal_booster.predict_two_class(x_test.to_numpy(), y=None) \n",
    "# rmse = np.sqrt(mean_squared_error(y_test.to_numpy(), prob_output))\n",
    "# print(\"RMSE(prob_output): %f\" % (rmse))\n",
    "\n",
    "preds = opt_focal_booster.predict(x_test.to_numpy(), y=None) \n",
    "rmse = np.sqrt(mean_squared_error(y_test.to_numpy(), preds))\n",
    "print(\"RMSE(preds): %f\" % (rmse))\n",
    "\n",
    "# retrieve the best parameters\n",
    "xgboost_opt_param = CV_focal_booster.best_params_\n",
    "# instantialize an imbalance-xgboost instance\n",
    "xgboost_opt = imb_xgb(special_objective='focal', num_round=200, max_depth=5, eta=0.1, **xgboost_opt_param)\n",
    "\n",
    "# # cross-validation\n",
    "# # initialize the splitter\n",
    "# loo_splitter = LeaveOneOut()\n",
    "# # initialize the score evalutation function by feeding the 'mode' argument\n",
    "# # 'mode' can be [\\'accuracy\\', \\'precision\\',\\'recall\\',\\'f1\\',\\'MCC\\']\n",
    "# score_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='accuracy')\n",
    "# # Leave-One cross validation\n",
    "# loo_info_dict = cross_validate(xgboost_opt, X=x_data.to_numpy(), y=y_data.to_numpy(), cv=loo_splitter, scoring=make_scorer(score_eval_func))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_opt.fit(x_train.to_numpy(), y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(x_test)\n",
    "prediction_output = xgboost_opt.boosting_model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_probs = xgboost_opt.predict(data_x=x_test , y=y_test)\n",
    "pred_probs = xgboost_opt.predict(data_x=x_test, y=y_test)\n",
    "\n",
    "# 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds 에 저장\n",
    "y_pred = [ 1 if x >= 0.5 else 0 for x in pred_probs ]\n",
    "get_clf_eval(y_test, y_pred, pred_probs)\n",
    "\n",
    "### @ save : plot tree & plot importance feature \n",
    "make_plot_tree(xgboost_opt.boosting_model, output_result_dir, outcome_name, rankdir=None)\n",
    "make_plot_tree(xgboost_opt.boosting_model, output_result_dir, outcome_name, rankdir='LR')\n",
    "make_plot_importance(xgboost_opt.boosting_model, output_result_dir, outcome_name)\n",
    "\n",
    "### @ save : clf report & model estimation & confusion matrix & roc\n",
    "clf_report(y_test, y_pred, output_result_dir, outcome_name)\n",
    "model_performance_evaluation(y_test, y_pred, pred_probs, output_result_dir, outcome_name)\n",
    "confusion_matrix_figure(y_test, y_pred, output_result_dir, outcome_name)\n",
    "confusion_matrix_figure2(y_test, y_pred, output_result_dir, outcome_name)\n",
    "AUC, ACC = ROC_AUC(y_test, y_pred, output_result_dir, outcome_name)\n",
    "\n",
    "### @ save : model json\n",
    "save_xgb_model_json(xgboost_opt.boosting_model, output_result_dir, outcome_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_opt_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_opt.correct_eval_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict[]\n",
    "print(f\"scoring is invalid (got {scoring!r}). Refer to the \"\n",
    "        \"scoring glossary for details: \"\n",
    "        \"https://scikit-learn.org/stable/glossary.html#term-scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [TP_eval_func, TN_eval_func, FP_eval_func, FN_eval_func]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='TP')\n",
    "TN_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='FP')\n",
    "FP_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='TN')\n",
    "FN_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='FN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"scoring is invalid (got {make_scorer(TP_eval_func)!r}). Refer to the \"\n",
    "        \"scoring glossary for details: \"\n",
    "        \"https://scikit-learn.org/stable/glossary.html#term-scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the correctness evalutation function by feeding the 'mode' argument\n",
    "# 'mode' can be ['TP', 'TN', 'FP', 'FN']\n",
    "TP_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='TP')\n",
    "TN_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='FP')\n",
    "FP_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='TN')\n",
    "FN_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='FN')\n",
    "# define the score function dictionary\n",
    "score_dict = {'TP': make_scorer(TP_eval_func), \n",
    "              'FP': make_scorer(TN_eval_func), \n",
    "              'TN': make_scorer(FP_eval_func), \n",
    "              'FN': make_scorer(FN_eval_func)}\n",
    "\n",
    "score_dict.__name__ = \"evalutation function\"\n",
    "# Leave-One cross validation\n",
    "loo_info_dict = cross_validate(xgboost_opt, X=x_data.to_numpy(), y=y_data.to_numpy(), cv=loo_splitter, scoring=score_dict)\n",
    "overall_tp = np.sum(loo_info_dict['test_TP']).astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "# class WrappablePartial(functools.partial):\n",
    "#     @property\n",
    "#     def __module__(self):\n",
    "#         return self.func.__module__\n",
    "#     @property\n",
    "#     def __name__(self):\n",
    "#         return \"functools.partial({}, *{}, **{})\".format(\n",
    "#             self.func.__name__,\n",
    "#             self.args,\n",
    "#             self.keywords\n",
    "#         )\n",
    "#     @property\n",
    "#     def __doc__(self):\n",
    "#         return self.func.__doc__\n",
    "   \n",
    "from functools import wraps\n",
    "\n",
    "TP_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='TP')\n",
    "TN_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='FP')\n",
    "FP_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='TN')\n",
    "FN_eval_func = functools.partial(xgboost_opt.score_eval_func, mode='FN')\n",
    "# define the score function dictionary\n",
    "score_dict = {'TP': make_scorer(TP_eval_func), \n",
    "              'FP': make_scorer(TN_eval_func), \n",
    "              'TN': make_scorer(FP_eval_func), \n",
    "              'FN': make_scorer(FN_eval_func)}\n",
    "# Leave-One cross validation\n",
    "loo_info_dict = cross_validate(xgboost_opt, X=x_data.to_numpy(), y=y_data.to_numpy(), cv=loo_splitter, scoring=score_dict)\n",
    "overall_tp = np.sum(loo_info_dict['test_TP']).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1, 'max_depth': 5, 'alpha': 10}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_opt.boosting_model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=x_train , label=y_train) \n",
    "dtest = xgb.DMatrix(data=x_test , label=y_test)\n",
    "\n",
    "params = { 'max_depth':5, 'learning_rate': 0.1, 'objective':'binary:logistic', 'eval_metric':'logloss' }\n",
    "num_rounds = 50\n",
    "# train 데이터 셋은 ‘train’ , evaluation(test) 데이터 셋은 ‘eval’ 로 명기합니다. \n",
    "wlist = [(dtrain,'train'),(dtest,'eval')]\n",
    "# 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달 \n",
    "xgb_model = xgb.train(params = params, dtrain=dtrain, num_boost_round=num_rounds, early_stopping_rounds=200, evals=wlist )\n",
    "pred_probs = xgb_model.predict(dtest)\n",
    "\n",
    "# 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds 에 저장\n",
    "y_pred = [ 1 if x >= 0.5 else 0 for x in pred_probs ]\n",
    "get_clf_eval(y_test, y_pred, pred_probs)\n",
    "\n",
    "make_plot_tree(xgb_model, output_result_dir, outcome_name, rankdir=None)\n",
    "make_plot_tree(xgb_model, output_result_dir, outcome_name, rankdir='LR')\n",
    "make_plot_importance(xgb_model, output_result_dir, outcome_name)\n",
    "\n",
    "clf_report(y_test, y_pred, output_result_dir, outcome_name)\n",
    "model_performance_evaluation(y_test, y_pred, pred_probs, output_result_dir, outcome_name)\n",
    "confusion_matrix_figure(y_test, y_pred, output_result_dir, outcome_name)\n",
    "confusion_matrix_figure2(y_test, y_pred, output_result_dir, outcome_name)\n",
    "AUC, ACC = ROC_AUC(y_test, y_pred, output_result_dir, outcome_name)\n",
    "\n",
    "save_xgb_model_json(xgb_model, output_result_dir, outcome_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMClassifier, plot_importance\n",
    "\n",
    "# def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n",
    "#     model.fit(ftr_train, tgt_train)\n",
    "#     pred = model.predict(ftr_test)\n",
    "#     pred_proba = model.predict_proba(ftr_test)[:, 1]\n",
    "#     get_clf_eval(tgt_test, pred, pred_proba)\n",
    "        \n",
    "#     fig, ax = plt.subplots(figsize=(10,12))\n",
    "#     plot_importance(model, ax=ax)\n",
    "#     plt.show()\n",
    "    \n",
    "# lgbm_clf = LGBMClassifier(n_estimators=400, num_leaves=10, n_jobs=-1, boost_from_average=False)\n",
    "# get_model_train_eval(lgbm_clf, ftr_train=x_train, ftr_test=x_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "#                 max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "\n",
    "# xg_reg.fit(x_train,y_train)\n",
    "\n",
    "# pred_probs = xg_reg.predict(x_test)     \n",
    "\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, pred_probs))\n",
    "# print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "# # 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds 에 저장\n",
    "# y_pred = [ 1 if x > 0.5 else 0 for x in pred_probs ]\n",
    "\n",
    "# data_dmatrix = xgb.DMatrix(data=x_data,label=y_data)\n",
    "\n",
    "# params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "#                 'max_depth': 5, 'alpha': 10}\n",
    "                \n",
    "# cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "#                     num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "                    \n",
    "# cv_results.head()\n",
    "# print((cv_results[\"test-rmse-mean\"]).tail(1))\n",
    "\n",
    "# xgb_model = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=100)\n",
    "\n",
    "# make_plot_tree(xgb_model, output_result_dir, outcome_name, rankdir=None)\n",
    "# make_plot_tree(xgb_model, output_result_dir, outcome_name, rankdir='LR')\n",
    "# make_plot_importance(xgb_model, output_result_dir, outcome_name)\n",
    "\n",
    "# clf_report(y_test, y_pred, output_result_dir, outcome_name)\n",
    "# model_performance_evaluation(y_test, y_pred, pred_probs, output_result_dir, outcome_name)\n",
    "# confusion_matrix_figure(y_test, y_pred, output_result_dir, outcome_name)\n",
    "# confusion_matrix_figure2(y_test, y_pred, output_result_dir, outcome_name)\n",
    "# AUC, ACC = ROC_AUC(y_test, y_pred, output_result_dir, outcome_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba9e2aafd4202ccdd325410855583e81bcee524d66662cf309288c7f44559fae"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
