{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/timeseriesAI/tsai/blob/master/tutorial_nbs/01_Intro_to_Time_Series_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "current_dir = pathlib.Path.cwd()\n",
    "parent_dir = current_dir.parent\n",
    "preprocessing_output_dir = pathlib.Path('{}/1_preprocessing_all_domain/output'.format(parent_dir))\n",
    "output_dir = pathlib.Path('{}/output'.format(current_dir))\n",
    "pathlib.Path.mkdir(output_dir, mode=0o777, exist_ok=True)\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/0_importsql/abnormal_list.txt'.format(parent_dir), 'r') as f:\n",
    "    data = f.read()\n",
    "outcome_name_list = data.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_y_data(df, OBP) :\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    y_data = df['label'].T.reset_index(drop=True) #df['label'].T.drop_duplicates().T.reset_index(drop=True)\n",
    "    y_data = np.array(y_data)\n",
    "    y_data = y_data[0:len(y_data):OBP].reshape(-1, 1).astype(int)\n",
    "    #print(len(y_data), file=_logfile_)\n",
    "\n",
    "    x_df = df.drop('label', axis=1)\n",
    "\n",
    "    # 2-d data to 3-d data\n",
    "    timestamp = OBP \n",
    "    x_data = np.array(x_df)\n",
    "    x_data = x_data.reshape(-1, timestamp, x_data.shape[1]) # -1(sample), timestamp, column\n",
    "    #x_data.shape, y_data.shape\n",
    "\n",
    "    # get Column data\n",
    "    new_col = x_df.columns\n",
    "    print(x_data.shape, y_data.shape, len(new_col))\n",
    "    return x_data, y_data, new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from Auto_lstm_attention import *\n",
    "c = Auto_lstm_attention()\n",
    "\n",
    "outcome_name = outcome_name_list[0]\n",
    "# 결과물 저장할 폴더 생성\n",
    "output_domain_path = pathlib.Path('{}/{}'.format(output_dir, outcome_name))\n",
    "pathlib.Path.mkdir(output_domain_path, mode=0o777, parents=True, exist_ok=True)\n",
    "        \n",
    "concat_df = pd.read_csv('{}/{}.txt'.format(preprocessing_output_dir, outcome_name), index_col=False)\n",
    "\n",
    "# ##### Case 1 : Split by subject_id #####\n",
    "# id_data = concat_df[['subject_id', 'label']].drop_duplicates().reset_index(drop=True)\n",
    "# x_id_data = np.array(id_data['subject_id'])\n",
    "# y_id_data = np.array(id_data['label'])\n",
    "\n",
    "# x_id_train, x_id_test, y_id_train, y_id_test = train_test_split(x_id_data, y_id_data, test_size=0.3, random_state=1, stratify=y_id_data) \n",
    "\n",
    "# train_df = concat_df[concat_df['subject_id'].isin(x_id_train)].reset_index(drop=True)\n",
    "# test_df = concat_df[concat_df['subject_id'].isin(x_id_test)].reset_index(drop=True)\n",
    "\n",
    "# train_df.to_csv('{}/{}_train.txt'.format(output_domain_path, outcome_name), index=False)\n",
    "# test_df.to_csv('{}/{}_test.txt'.format(output_domain_path, outcome_name), index=False)\n",
    "\n",
    "# concat_df = concat_df.drop(['subject_id', 'unique_id', 'cohort_start_date', 'concept_date', 'first_abnormal_date'], axis=1)\n",
    "# train_df = train_df.drop(['subject_id', 'unique_id', 'cohort_start_date', 'concept_date', 'first_abnormal_date'], axis=1)\n",
    "# test_df = test_df.drop(['subject_id', 'unique_id', 'cohort_start_date', 'concept_date', 'first_abnormal_date'], axis=1)\n",
    "\n",
    "# x_data, y_data, new_col = split_x_y_data(concat_df, OBP=28)\n",
    "# x_train, y_train, new_col = split_x_y_data(train_df, OBP=28)\n",
    "# x_test, y_test, new_col = split_x_y_data(test_df, OBP=28)\n",
    "\n",
    "#### Case 2 : Split ignore subject_id #####\n",
    "concat_df = concat_df.drop(['subject_id', 'unique_id', 'cohort_start_date', 'concept_date', 'first_abnormal_date'], axis=1)\n",
    "x_data, y_data, new_col = split_x_y_data(concat_df, OBP=28)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=1, stratify=y_data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## NOTE: UNCOMMENT AND RUN THIS CELL IF YOU NEED TO INSTALL/ UPGRADE TSAI\n",
    "# stable = False # True: stable version in pip, False: latest version from github\n",
    "# if stable: \n",
    "#     !pip install tsai -U >> /dev/null\n",
    "# else:      \n",
    "#     !pip install git+https://github.com/timeseriesAI/tsai.git -U >> /dev/null\n",
    "# ## NOTE: REMEMBER TO RESTART (NOT RECONNECT/ RESET) THE KERNEL/ RUNTIME ONCE THE INSTALLATION IS FINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.all import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computer_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test  = get_UCR_data(dsid, return_split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use this convenience function to get X, y and splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, splits = combine_split_data([x_train, x_test], [y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create datasets. This is very easy to do in v2. \n",
    "\n",
    "In TS classification problems, you will usually want to use an item tfm to transform y into categories.\n",
    "\n",
    "We'll use inplace=True to preprocess data at dataset initialization. This will significantly speed up training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms  = [None, [Categorize()]]\n",
    "dsets = TSDatasets(x, y.flatten(), tfms=tfms, splits=splits, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now build the dataloaders that will create batches of data.\n",
    "\n",
    "You will need to pass:\n",
    "\n",
    "* datasets: usually 2 - train and valid -  or 1 - test or unlabeled- depending on the problem\n",
    "* batch size(s): you may pass a single value (will will be appied to all dls, or different values, one for each dl.\n",
    "* batch_tfms (same as after_batch): you may decide to pass some tfms at the batch level. In this case for example, we'll standardize the data (0 mean and 1 std). You may get more details on how these transforms work in the transforms nb.\n",
    "* num workers: num_workers > 0 is used to preprocess batches of data so that the next batch is ready for use when the current batch has been finished. More num_workers would consume more memory usage but is helpful to speed up the I/O process. This will depend on your machine, dataset, etc. You may want to start with 0, and test other values to see how to train faster. For me, 0 works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[32, 64], batch_tfms=[TSStandardize()], num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build learner ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTime(dls.vars, dls.c)\n",
    "learn = Learner(dls, model, metrics=accuracy)\n",
    "learn.save('stage0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model ?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR find ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage0')\n",
    "learn.lr_find()\n",
    "#learn.lr_find(suggestions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ?????��?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(25, lr_max=1e-3)\n",
    "learn.save('stage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend we need to end the working session now for some reason, but we'd like to continue working with this datasets and learner in the future. \n",
    "\n",
    "To save everything you can use a convenience function I've created that saves the learner with the model, the data and the opt function status: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_all(path='export', dls_fname='dls', model_fname='model', learner_fname='learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as we've done this, we can end the session, and continue at any time in the future. \n",
    "\n",
    "Let's simulate that we need to end the session now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del learn, dsets, dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time we go back to work, we'll need to reload the datasets and learner (with the same status we had):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner_all(path='export', dls_fname='dls', model_fname='model', learner_fname='learner')\n",
    "dls = learn.dls\n",
    "valid_dl = dls.valid\n",
    "b = next(iter(valid_dl))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_probas, valid_targets, valid_preds = learn.get_preds(dl=valid_dl, with_decoded=True)\n",
    "valid_probas, valid_targets, valid_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the learner has the same status it had at the end of training, by confirming the validation accuracy is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(valid_targets == valid_preds).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It's the same. This means we have now the learner at the same point where we left it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_probas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.most_confused(min_val=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on additional data ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to predict labels on new data. Let's see how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may have additional data (test set) where we want to check our performance. In this case, we'd add a labeled dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled test data\n",
    "test_ds = valid_dl.dataset.add_test(x, y.flatten())# In this case I'll use X and y, but this would be your test data\n",
    "test_dl = valid_dl.new(test_ds)\n",
    "next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By selecting the valid dataset (valid_dl.dataset) we ensure that the same tfms applied to the valid data will be applied to the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probas, test_targets, test_preds = learn.get_preds(dl=test_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
    "test_probas, test_targets, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy: {skm.accuracy_score(test_targets, test_preds):10.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data is unlabeled, we'd just do this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlabeled data\n",
    "test_ds = dls.dataset.add_test(x)\n",
    "test_dl = valid_dl.new(test_ds)\n",
    "next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probas, *_ = learn.get_preds(dl=test_dl, save_preds=None)\n",
    "test_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the code you need to train a TS model. As you can see, it's v2 is easier to use and faster compared to v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsid = 'NATOPS' \n",
    "X, y, splits = get_UCR_data(dsid, return_split=False)\n",
    "tfms  = [None, [Categorize()]]\n",
    "dsets = TSDatasets(X, y, tfms=tfms, splits=splits, inplace=True)\n",
    "dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], batch_tfms=[TSStandardize()], num_workers=0)\n",
    "model = InceptionTime(dls.vars, dls.c)\n",
    "learn = Learner(dls, model, metrics=accuracy)\n",
    "learn.fit_one_cycle(25, lr_max=1e-3)\n",
    "learn.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New scikit-learn-like API ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of `tsai` version 0.2.15 I have added a new scikit-learn-like API to further simplify the learner creation. \n",
    "\n",
    "I will prepare a new tutorial to further demonstrate how you can use the new API.\n",
    "\n",
    "This is how you can use it for Time Series Classification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsid = 'NATOPS' \n",
    "X, y, splits = get_UCR_data(dsid, return_split=False)\n",
    "learn = TSClassifier(X, y, splits=splits, bs=[64, 128], batch_tfms=[TSStandardize()], arch=InceptionTime, metrics=accuracy)\n",
    "learn.fit_one_cycle(25, lr_max=1e-3)\n",
    "learn.plot_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
