{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** import package **\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "from _utils.model_estimation import *\n",
    "from _utils.customlogger import customlogger as CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** loading config **\n",
    "with open('./../{}'.format(\"config.json\")) as file:\n",
    "    cfg = json.load(file)\n",
    "with open('./../{}'.format(\"config_params.json\")) as file:\n",
    "    params = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** loading info **\n",
    "current_dir = pathlib.Path.cwd()\n",
    "parent_dir = current_dir.parent\n",
    "current_date = cfg[\"working_date\"]\n",
    "curr_file_name = os.path.splitext(os.path.basename(os.path.abspath('')))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **create Logger**\n",
    "log = CL(\"custom_logger\")\n",
    "pathlib.Path.mkdir(pathlib.Path('{}/_log/'.format(parent_dir)), mode=0o777, parents=True, exist_ok=True)\n",
    "log = log.create_logger(file_name=\"../_log/{}.log\".format(curr_file_name), mode=\"a\", level=\"DEBUG\")  \n",
    "log.debug('start {}'.format(curr_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from imv_lstm_model import IMVFullLSTM\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linux\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outcome_name in tqdm(cfg['drug'].keys()) :\n",
    "    try :\n",
    "        # In[ ]:\n",
    "        ps_data_dir = pathlib.Path('{}/data/{}/preprocess_lstm/{}/'.format(parent_dir, current_date, outcome_name))\n",
    "        output_dir = pathlib.Path('{}/result/{}/imv_lstm_attention/{}/'.format(parent_dir, current_date, outcome_name))\n",
    "        pathlib.Path.mkdir(output_dir, mode=0o777, parents=True, exist_ok=True)\n",
    "\n",
    "        def split_x_y_data(df, OBP) :\n",
    "            import numpy as np\n",
    "            import pandas as pd\n",
    "\n",
    "            y_data = df['label'].T.reset_index(drop=True) #df['label'].T.drop_duplicates().T.reset_index(drop=True)\n",
    "            y_data = np.array(y_data)\n",
    "            y_data = y_data[0:len(y_data):OBP].reshape(-1, 1).astype(int)\n",
    "            #print(len(y_data), file=_logfile_)\n",
    "\n",
    "            X_df = df.drop('label', axis=1)\n",
    "\n",
    "            # 2-d data to 3-d data\n",
    "            timestamp = OBP \n",
    "            X_data = np.array(X_df)\n",
    "            X_data = X_data.reshape(-1, timestamp, X_data.shape[1]) # -1(sample), timestamp, column\n",
    "            #X_data.shape, y_data.shape\n",
    "\n",
    "            # get Column data\n",
    "            new_col = X_df.columns\n",
    "            print(X_data.shape, y_data.shape, len(new_col))\n",
    "            return X_data, y_data, new_col\n",
    "            \n",
    "        concat_df = pd.read_csv('{}/{}.txt'.format(ps_data_dir, outcome_name), index_col=False)\n",
    "\n",
    "        # ##### Case 1 : Split by person_id #####\n",
    "        # id_data = concat_df[['person_id', 'label']].drop_duplicates().reset_index(drop=True)\n",
    "        # x_id_data = np.array(id_data['person_id'])\n",
    "        # y_id_data = np.array(id_data['label'])\n",
    "        \n",
    "        # x_id_train, x_id_test, y_id_train, y_id_test = train_test_split(x_id_data, y_id_data, test_size=0.3, random_state=1, stratify=y_id_data) \n",
    "        \n",
    "        # train_df = concat_df[concat_df['person_id'].isin(x_id_train)].reset_index(drop=True)\n",
    "        # test_df = concat_df[concat_df['person_id'].isin(x_id_test)].reset_index(drop=True)\n",
    "        \n",
    "        # train_df.to_csv('{}/{}_train.txt'.format(output_dir, outcome_name), index=False)\n",
    "        # test_df.to_csv('{}/{}_test.txt'.format(output_dir, outcome_name), index=False)\n",
    "        \n",
    "        # concat_df = concat_df.drop(['person_id', 'unique_id', 'cohort_start_date', 'concept_date', 'first_abnormal_date'], axis=1)\n",
    "        # train_df = train_df.drop(['person_id', 'unique_id', 'cohort_start_date', 'concept_date', 'first_abnormal_date'], axis=1)\n",
    "        # test_df = test_df.drop(['person_id', 'unique_id', 'cohort_start_date', 'concept_date', 'first_abnormal_date'], axis=1)\n",
    "        \n",
    "        # x_data, y_data, new_col = split_x_y_data(concat_df, OBP=28)\n",
    "        # x_train, y_train, new_col = split_x_y_data(train_df, OBP=28)\n",
    "        # x_test, y_test, new_col = split_x_y_data(test_df, OBP=28)\n",
    "        \n",
    "        #### Case 2 : Split ignore person_id #####\n",
    "        concat_df = concat_df.drop(['person_id', 'unique_id', 'cohort_start_date', 'concept_date', 'first_abnormal_date'], axis=1)\n",
    "\n",
    "        X_data, y_data, cols = split_x_y_data(concat_df, OBP=params['windowsize'])\n",
    "        cols = [textwrap.shorten(col, width=50, placeholder=\"...\") for col in cols]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=1, stratify=y_data) \n",
    "\n",
    "        y_train = y_train.reshape(-1)\n",
    "        y_test = y_test.reshape(-1)\n",
    "        train_bound = int(0.8*(len(X_train)))\n",
    "\n",
    "        X_val = X_train[train_bound:]\n",
    "        X_train = X_train[:train_bound]\n",
    "        y_val = y_train[train_bound:]\n",
    "        y_train = y_train[:train_bound]\n",
    "        depth = params['windowsize']\n",
    "\n",
    "        X_train_min, X_train_max = X_train.min(axis=0), X_train.max(axis=0)\n",
    "        y_train_min, y_train_max = y_train.min(axis=0), y_train.max(axis=0)\n",
    "\n",
    "        X_train_t = torch.Tensor(X_train)\n",
    "        X_val_t = torch.Tensor(X_val)\n",
    "        X_test_t = torch.Tensor(X_test)\n",
    "        y_train_t = torch.Tensor(y_train)\n",
    "        y_val_t = torch.Tensor(y_val)\n",
    "        y_test_t = torch.Tensor(y_test)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=64, shuffle=False)\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            print(y.shape)\n",
    "            break\n",
    "\n",
    "        model = IMVFullLSTM(X_train_t.shape[2], 1, 128)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=params['learningrate'])\n",
    "        epoch_scheduler = torch.optim.lr_scheduler.StepLR(opt, 20, gamma=0.9)\n",
    "        \n",
    "        epochs = params[\"epochs\"]\n",
    "        patience = params[\"patience\"]\n",
    "        min_val_loss = params[\"min_val_loss\"]\n",
    "        loss = nn.MSELoss()\n",
    "        counter = 0\n",
    "        for i in range(epochs):\n",
    "            mse_train = 0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x = batch_x\n",
    "                batch_y = batch_y\n",
    "                opt.zero_grad()\n",
    "                y_pred, alphas, betas = model(batch_x)\n",
    "                y_pred = y_pred.squeeze(1)\n",
    "                l = loss(y_pred, batch_y)\n",
    "                l.backward()\n",
    "                mse_train += l.item()*batch_x.shape[0]\n",
    "                opt.step()\n",
    "            epoch_scheduler.step()\n",
    "            with torch.no_grad():\n",
    "                mse_val = 0\n",
    "                preds = []\n",
    "                true = []\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x = batch_x\n",
    "                    batch_y = batch_y\n",
    "                    output, alphas, betas = model(batch_x)\n",
    "                    output = output.squeeze(1)\n",
    "                    preds.append(output.detach().cpu().numpy())\n",
    "                    true.append(batch_y.detach().cpu().numpy())\n",
    "                    mse_val += loss(output, batch_y).item()*batch_x.shape[0]\n",
    "            preds = np.concatenate(preds)\n",
    "            true = np.concatenate(true)\n",
    "            \n",
    "            if min_val_loss > mse_val**0.5:\n",
    "                min_val_loss = mse_val**0.5\n",
    "                print(\"Saving...\")\n",
    "                torch.save(model.state_dict(), \"{}/{}_model_state_dict.pt\".format(output_dir, outcome_name))\n",
    "                counter = 0\n",
    "            else: \n",
    "                counter += 1\n",
    "            \n",
    "            if counter == patience:\n",
    "                break\n",
    "            print(\"Iter: \", i, \"train: \", (mse_train/len(X_train_t))**0.5, \"val: \", (mse_val/len(X_val_t))**0.5)\n",
    "            if(i % 10 == 0):\n",
    "                preds = preds*(y_train_max - y_train_min) + y_train_min\n",
    "                true = true*(y_train_max - y_train_min) + y_train_min\n",
    "                mse = mean_squared_error(true, preds)\n",
    "                mae = mean_absolute_error(true, preds)\n",
    "                print(\"lr: \", opt.param_groups[0][\"lr\"])\n",
    "                print(\"mse: \", mse, \"mae: \", mae)\n",
    "                plt.figure(figsize=(20, 10))\n",
    "                plt.plot(preds)\n",
    "                plt.plot(true)\n",
    "                plt.show()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mse_val = 0\n",
    "            preds = []\n",
    "            true = []\n",
    "            alphas = []\n",
    "            betas = []\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x = batch_x\n",
    "                batch_y = batch_y\n",
    "                aoutput, a, b = model(batch_x)\n",
    "                output = output.squeeze(1)\n",
    "                preds.append(output.detach().cpu().numpy())\n",
    "                true.append(batch_y.detach().cpu().numpy())\n",
    "                alphas.append(a.detach().cpu().numpy())\n",
    "                betas.append(b.detach().cpu().numpy())\n",
    "                mse_val += loss(output, batch_y).item()*batch_x.shape[0]\n",
    "        preds = np.concatenate(preds)\n",
    "        true = np.concatenate(true)\n",
    "        preds = preds*(y_train_max - y_train_min) + y_train_min\n",
    "        true = true*(y_train_max - y_train_min) + y_train_min\n",
    "        mse = mean_squared_error(true, preds)\n",
    "        mae = mean_absolute_error(true, preds)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.plot(preds)\n",
    "        plt.plot(true)\n",
    "        plt.show()\n",
    "        alphas = np.concatenate(alphas)\n",
    "        betas = np.concatenate(betas)\n",
    "        alphas = alphas.mean(axis=0)\n",
    "        betas = betas.mean(axis=0)\n",
    "        alphas = alphas[..., 0]\n",
    "        betas = betas[..., 0]\n",
    "        alphas = alphas.transpose(1, 0)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(40, 30))\n",
    "        im = ax.imshow(alphas)\n",
    "        ax.set_xticks(np.arange(X_train_t.shape[1]))\n",
    "        ax.set_yticks(np.arange(len(cols)))\n",
    "        ax.set_xticklabels([\"t-\"+str(i) for i in np.arange(X_train_t.shape[1], 0, -1)])\n",
    "        ax.set_yticklabels(cols)\n",
    "        for i in range(len(cols)):\n",
    "            for j in range(X_train_t.shape[1]):\n",
    "                text = ax.text(j, i, round(alphas[i, j], 3),\n",
    "                            ha=\"center\", va=\"center\", color=\"w\")\n",
    "        ax.set_title(\"Importance of features and timesteps\")\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        plt.savefig('{}/{}_heatmap_.png'.format(output_dir, outcome_name), format='png',\n",
    "                            dpi=300, facecolor='white', transparent=True,  bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 15))\n",
    "        plt.title(\"Feature importance\")\n",
    "        plt.barh(cols, betas)\n",
    "        plt.gca().invert_yaxis()\n",
    "        # plt.xticks(ticks=range(len(cols)), labels=list(cols), rotation=90)\n",
    "\n",
    "        plt.savefig('{}/{}_Feature_importance_.png'.format(output_dir, outcome_name), format='png',\n",
    "                            dpi=300, facecolor='white', transparent=True,  bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        y_true = true\n",
    "        y_pred_proba = preds\n",
    "        y_pred = np.rint(preds)\n",
    "\n",
    "        confusion_matrix_figure2(y_true, y_pred, output_dir, outcome_name)\n",
    "        ROC_AUC(y_pred_proba, y_true, output_dir, outcome_name)\n",
    "        model_performance_evaluation(y_true, y_pred, y_pred_proba, output_dir, outcome_name)\n",
    "\n",
    "    except :\n",
    "        traceback.print_exc()\n",
    "        log.error(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tsai_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64f02459bd63abfac701186b3215662d45c7eb2d9d5cc5e08f5fbdc6e91ab167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
